{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"../vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Knowledge base and Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .md files from: '../knowledge_base' with doc_type: 'knowledge_base'\n"
     ]
    }
   ],
   "source": [
    "knowledge_base_dir = '../knowledge_base'\n",
    "current_doc_type = os.path.basename(os.path.normpath(knowledge_base_dir))\n",
    "print(f\"Loading .md files from: '{knowledge_base_dir}' with doc_type: '{current_doc_type}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metadata(document, doc_type):\n",
    "    document.metadata[\"doc_type\"] = doc_type\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 62.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 5 document(s) from '../knowledge_base'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "try:\n",
    "    # Initialize DirectoryLoader to load .md files from the knowledge_base_dir\n",
    "    loader = DirectoryLoader(\n",
    "        path=knowledge_base_dir,          # Path to the directory to search\n",
    "        glob=\"*.md\",                      # Pattern to match files (e.g., files.md)\n",
    "        loader_cls=TextLoader,            # Loader to use for .md files\n",
    "        loader_kwargs={'encoding': 'utf-8'}, # Arguments for TextLoader\n",
    "        show_progress=True,               # Optional: shows a progress bar\n",
    "        use_multithreading=False,         # Optional: set to True for potential speedup with many files\n",
    "                                          # recursive=False by default, which is what we want here.\n",
    "    )\n",
    "\n",
    "    # Load the documents\n",
    "    folder_documents = loader.load()\n",
    "\n",
    "    if folder_documents:\n",
    "        print(f\"Successfully loaded {len(folder_documents)} document(s) from '{knowledge_base_dir}'.\")\n",
    "        # Add metadata to each loaded document\n",
    "        for doc in folder_documents:\n",
    "            documents.append(add_metadata(doc, current_doc_type))\n",
    "    else:\n",
    "        print(f\"No .md documents found in '{knowledge_base_dir}'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The directory '{knowledge_base_dir}' was not found. Please check the path.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during document loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 32\n",
      "Document types found: {'knowledge_base'}\n"
     ]
    }
   ],
   "source": [
    "# Split documents into smaller chunks\n",
    "text_splitter = MarkdownTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunked_documents)}\")\n",
    "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\venna\\AppData\\Local\\Temp\\ipykernel_17656\\1548711557.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 32 documents\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "vectorstore = Chroma.from_documents(documents=chunked_documents, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LLM and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\venna\\AppData\\Local\\Temp\\ipykernel_17656\\949353121.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0, api_key=API_KEY)\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup query expansion and cross-encoder re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    A retriever that combines query expansion and cross-encoder re-ranking.\n",
    "    \"\"\"\n",
    "    vectorstore_retriever: VectorStoreRetriever\n",
    "    llm: BaseChatModel\n",
    "    top_k: int = 5\n",
    "\n",
    "    def _get_relevant_documents(self, query: str, *, run_manager):\n",
    "        reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "        expansion_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"You are an AI assistant. Your task is to take a user's question and generate 3 different versions of it to improve document retrieval.\n",
    "            Provide only the reformulated questions, separated by newlines.\n",
    "            Original Question: {question}\"\"\"\n",
    "        )\n",
    "        expansion_chain = expansion_prompt | self.llm | StrOutputParser()\n",
    "        expanded_queries_str = expansion_chain.invoke({\"question\": query}, config={\"run_name\": \"QueryExpansion\"})\n",
    "        all_queries = [query] + expanded_queries_str.strip().split('\\n')\n",
    "        \n",
    "        all_retrieved_docs = []\n",
    "        for q in all_queries:\n",
    "            # Retrieve documents for each expanded query\n",
    "            all_retrieved_docs.extend(self.vectorstore_retriever.get_relevant_documents(q))\n",
    "\n",
    "        unique_docs_dict = {doc.page_content: doc for doc in all_retrieved_docs}\n",
    "        unique_docs = list(unique_docs_dict.values())\n",
    "        \n",
    "        if not unique_docs:\n",
    "            return []\n",
    "\n",
    "        doc_texts = [doc.page_content for doc in unique_docs]\n",
    "        query_doc_pairs = [[query, doc_text] for doc_text in doc_texts]\n",
    "        \n",
    "        scores = reranker.predict(query_doc_pairs)\n",
    "        \n",
    "        doc_scores = list(zip(unique_docs, scores))\n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        reranked_docs = [doc for doc, score in doc_scores[:self.top_k]]\n",
    "        \n",
    "        return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_retriever = AdvancedRetriever(\n",
    "    vectorstore_retriever=retriever, \n",
    "    llm=llm,\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup QA prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define system prompt content\n",
    "system_prompt_content = \"\"\"\n",
    "You are a specialized AI assistant. I am your builder, and I have a knowledge base that contains information about my qualifications, projects, and other relevant details. Your role is to assist recruiters and HR professionals in understanding my background and expertise.\n",
    "You will answer questions about me, your builder, you can refer to me as 'My Builder', based solely on the information provided in the context documents from my knowledge base. You must not use any external knowledge or make assumptions beyond what is explicitly stated in those documents.\n",
    "Your dedicated role is to assist recruiters and HR professionals. In your conversation, the person you are talking to (the recruiter or HR professional) will be referred to as 'you'.\n",
    "\n",
    "It is absolutely crucial to understand that 'Builder' IS NOT the person you are currently interacting with.\n",
    "Therefore, you MUST NOT use phrases that equate or confuse 'Builder' with 'you' (the recruiter/HR). For example, do not say 'Builder (you)', 'your projects as Builder', or any similar phrasing that implies the recruiter is Builder. 'Builder' is strictly the subject of the knowledge base.\n",
    "\n",
    "Your answers must be based STRICTLY and ONLY on the information contained in the provided context documents from Builder's knowledge base.\n",
    "When discussing Builder's qualifications, projects, or any other information, consistently use the name 'Builder' or 'My Builder'. For example: 'Builder has expertise in...' or 'This project was undertaken by Builder.'\n",
    "\n",
    "If the information needed to answer a question is not present in the provided context, you MUST clearly state: 'I am unable to find that specific information about Builder in the provided documents.'\n",
    "Under no circumstances should you use external knowledge, make assumptions, or generate information not explicitly present in the context.\n",
    "Your responses must be concise, factual, and maintain a professional and helpful tone when addressing the recruiter or HR professional (i.e., 'you').\n",
    "If the provided context is empty or entirely irrelevant to the question asked, respond with: 'I cannot answer that question based on the provided documents about Builder.'\n",
    "\"\"\"\n",
    "# define prompt template\n",
    "# The chat_history is handled by ConversationalRetrievalChain to condense the question before this prompt.\n",
    "qa_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt_content),\n",
    "    (\"human\", \"Given the following context and question, please provide an answer.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Condense Question Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom prompt template\n",
    "condense_question_template = \"\"\"\n",
    "Given the following chat history and a follow-up question, rephrase the follow-up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "\n",
    "# Instantiate the prompt template\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(condense_question_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it together: set up the conversation chain with the LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    retriever=advanced_retriever, \n",
    "    memory=memory, \n",
    "    combine_docs_chain_kwargs={\"prompt\": qa_prompt_template},\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Gradio Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_fit_css = \"\"\"\n",
    "#screen_fit_chatbot {\n",
    "    height: 78vh !important; /* Main height for the chatbot message display area */\n",
    "    display: flex !important; /* Use flexbox for internal layout */\n",
    "    flex-direction: column !important;\n",
    "}\n",
    "#screen_fit_chatbot > .wrap { /* Targets the inner container that holds messages */\n",
    "    flex-grow: 1 !important; /* Allows this area to expand to fill the specified height */\n",
    "    overflow-y: auto !important; /* Makes the message area scrollable if content exceeds height */\n",
    "    min-height: 0 !important; /* Important for flex-grow to work correctly in various content scenarios */\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bot_greeting = \"Hello! You can ask me questions about my builder's experiences and projects. What would you like to know?\"\n",
    "custom_chatbot_instance = gr.Chatbot(\n",
    "    elem_id=\"screen_fit_chatbot\",\n",
    "    value=[{\"role\": \"assistant\", \"content\": initial_bot_greeting}], # Initial greeting in \"messages\" format\n",
    "    label=\"RAG Chatbot\",\n",
    "    bubble_full_width=True,\n",
    "    type=\"messages\"  # Explicitly set the type for the Chatbot instance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\venna\\AppData\\Local\\Temp\\ipykernel_17656\\2462316134.py:24: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  all_retrieved_docs.extend(self.vectorstore_retriever.get_relevant_documents(q))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "view = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    chatbot=custom_chatbot_instance,\n",
    "    type=\"messages\",\n",
    "    css=screen_fit_css\n",
    ").launch(inbrowser=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
